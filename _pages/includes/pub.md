
# 📝 Publications 
## 国际会议
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/TCDiffpp_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[arXiv25][TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography](https://arxiv.org/pdf/2506.18671)
 
<a href="https://github.com/Da1yuqin/TCDiffpp"><img src="https://img.shields.io/github/stars/Da1yuqin/TCDiffpp?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Da1yuqin/TCDiffpp"><img src="https://img.shields.io/github/forks/Da1yuqin/TCDiffpp?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Da1yuqin/TCDiff) [[Project Page]](https://da1yuqin.github.io/TCDiffpp.website/)

 **Yuqin Dai***, Wanlu Zhu*, Ronghui Li, Xiu Li, Zhenyu Zhang, Jun Li,Jian Yang
- 我们提出TCDiff++, 一种端到端版本的群舞生成模型。
- 我们引入位置嵌入和一致性损失，防止碰撞并保持合理间距。
- 模型加入换位信息和脚步自适应器，减少脚滑并提升一致性。
- 优化长时生成效果，提出长序列采样与解码器，优化长舞蹈生成的连贯性。
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/realfactbentch_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[arXiv25][RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking](https://arxiv.org/pdf/2506.12538)

Shuo Yang, **Yuqin Dai**, Guoqing Wang, Xinran Zheng, Jinfeng Xu, Jinze Li, Zhenzhe Ying, Weiqiang Wang, Edith CH Ngai.
- 我们提出 RealFactBench 基准测试集：构建了一个涵盖知识验证、谣言检测和事件核查等多种真实世界任务的综合性基准，用于评估大语言模型（LLMs）和多模态大模型（MLLMs）的事实核查能力。
- 引入新的评估指标 Unknown Rate (UnR)：该指标用于更细致地衡量模型在不确定性处理方面的表现，帮助评估模型在保守性与自信程度之间的平衡。
- 开展大规模实证研究：在7个典型LLMs和4个MLLMs上进行了系统评估，揭示了当前模型在事实核查任务中的局限性，并为后续研究提供了有价值的洞察。
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2025</div><img src='images/mindaligner_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ICML25][MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data](https://arxiv.org/pdf/2502.05034)
 
<a href="https://github.com/Da1yuqin/MindAligner"><img src="https://img.shields.io/github/stars/Da1yuqin/MindAligner?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Da1yuqin/MindAligner"><img src="https://img.shields.io/github/forks/Da1yuqin/MindAligner?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Da1yuqin/TCDiff) 
 
**Yuqin Dai**\*, Zhouheng Yao\*, Chunfeng Song, Qihao Zheng, Weijian Mai, Kunyu Peng, Shuai Lu, Wanli Ouyang, Jian Yang, Jiamin Wu.
- 我们提出了MindAligner，这是第一个显式的大脑对齐框架，能够在数据有限的情况下实现跨个体的视觉解码和大脑功能分析。
- 我们提出了一种大脑转移矩阵，用于建立不同个体之间的细粒度功能对应关系。该矩阵通过大脑功能对齐模块进行优化，采用多层次对齐损失实现软性跨个体映射。
- 实验表明，MindAligner在视觉解码任务中，只有6%的模型参数被学习时，便超越了现有的最先进方法。
- 我们进行了跨个体的大脑功能可视化研究，发现早期视觉皮层在不同个体间活动相似，而与记忆和空间导航相关的高级视觉皮层则表现出显著的个体间差异。


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2025</div><img src='images/hvsurvey_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[TPAMI25][Human Motion Video Generation: A Survey](https://www.techrxiv.org/users/836049/articles/1228135-human-motion-video-generation-a-survey)

<a href="https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file"><img src="https://img.shields.io/github/stars/Winn1y/Awesome-Human-Motion-Video-Generation?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file"><img src="https://img.shields.io/github/forks/Winn1y/Awesome-Human-Motion-Video-Generation?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file) [[Project Page]](https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file)

Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,__Yuqin Dai__,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu. 
- 数字人视频生成领域综述. 
- 总结了超过300篇最新数字人视频生成了领域相关文献的内容.
- 总结了现有数字人视频生成领域范式.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI2025 Oral</div><img src='images/TCDiff_mainfig.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[AAAI25 Oral][Harmonious Group Choreography with Trajectory-Controllable Diffusion](https://arxiv.org/pdf/2403.06189)

<a href="https://github.com/Da1yuqin/TCDiff"><img src="https://img.shields.io/github/stars/Da1yuqin/TCDiff?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Da1yuqin/TCDiff"><img src="https://img.shields.io/github/forks/Da1yuqin/TCDiff?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Da1yuqin/TCDiff) [[Project Page]](https://wanluzhu.github.io/TCDiffusion/)

__Yuqin Dai__, Wanlu Zhu, Ronghui Li, Zeping Ren, Xiangzheng Zhou, Xiu Li, Jun Li, Jian Yang. 
- 发现并提出领域内存在的问题: 舞者混淆(Dancer Ambiguity)现象. 为后续研究提供指引与思路. 
- TCDiff 是当前 SOTA 的多人舞蹈生成模型, 能够较好的解决舞者混淆(Dancer Ambiguity)现象.
- 提出了 Footwork Adaptor 模块, 能有效缓解多人舞蹈生成中的脚步滑动问题(Foot Slide).
- 提出了 Fusion Projection 插件, 该插件占用较小的计算资源, 能够有效解决舞者混淆(Dancer Ambiguity)现象

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP2024 Oral</div><img src='images/mainfig_text2avatar.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ICASSP24 Oral][Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute](https://ieeexplore.ieee.org/document/10446237)

 Chaoqun Gong\*, __Yuqin Dai__\*, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu L. 
- Text2Avatar 是第一个基于复杂耦合的输入文本提示生成逼真风格的 3D Avatar 的模型，实现了多属性可控和逼真的 3D 人头像生成。
- Text2Avatar 模型基于 3D-Aware GAN(NeRF-Based), 使用 GAN-Inversion based 的方式实现文本对齐, 巧妙化解了当前文本标注的写实风格三维 Avatar 数据集缺失的问题.
- 提出 Multi-Modal Encoder, 能够作为插件服务于其他模型, 具有很强的可扩展性.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP2024</div><img src='images/DanceControl_mainfig.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ICASSP24][EXPLORING MULTI-MODAL CONTROL IN MUSIC-DRIVEN DANCE GENERATION](https://ieeexplore.ieee.org/document/10447825)

Ronghui Li\*, __Yuqin Dai__\*, Yachao Zhang, Jun Li, Jian Yang, Jie Guo, Xiu Li. 
- 提出了第一个统一的框架，能够生成高质量的舞蹈动作，并支持多模态控制，包括同时进行流派控制、语义控制和空间控制。
- 模型能够进行音乐跨模态舞蹈生成(Music2Dance), 基于 VQ-GPT 架构, 能够一次生成长达 16s 的舞蹈动作, 并能通过自回归的方式对生成内容进行拓展. 

</div>
</div>

<!-- 
## 中文论文
[基于Encoder-Decoder注意力网络的异常驾驶行为在线识别方法](http://bzxb.cqut.edu.cn/paperinfo.aspx?paperid=11163)，兵器装备工程学报2023(核心期刊) 
- 唐坤(指导老师)，__戴语琴__，徐永能，郭唐仪，邵飞
- 我们提出了一个实时监控驾驶员异常驾驶行为的框架。与现有依赖大量昂贵高精度传感器的方法不同，我们只使用手机数据进行异常驾驶行为检测，使我们的方法更加实用。

[转动惯量平行轴定理验证实验的改进方案](https://dxwl.bnu.edu.cn/CN/10.16854%20/j.cnki.1000-0712.190392)，大学物理2020(核心期刊)
- 闫敏，__戴语琴__，袁俊，王晓雄
- 我们找到了一种比目前的教学方法更准确的方法，可以显着减少实验中测量刚体转动惯量时引入的误差量。这将有助于大学生更有效地进行实验，更好地理解实验原理。
-->


## 专利
- 唐坤， 杨力， __戴语琴__，郭唐仪，徐永能. [基于编码器-解码器注意力网络和LSTM的异常驾驶在线识别方法](https://xueshu.baidu.com/usercenter/paper/show?paperid=180r0c70ap6800w0hu2t0jn0je340821#:~:text=%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E6%98%AF%E8%BD%A6%E8%BE%86%E5%AE%89%E5%85%A8%E8%BF%90%E8%A1%8C%E7%9A%84%E9%87%8D%E5%A4%A7%E5%A8%81%E8%83%81%2C%E5%85%B6%E5%AF%B9%E4%BA%BA%E5%91%98%E4%B8%8E%E7%89%A9%E8%B5%84%E7%9A%84%E5%AE%89%E5%85%A8%E9%AB%98%E6%95%88%E6%8A%95%E9%80%81%E9%80%A0%E6%88%90%E4%B8%A5%E9%87%8D%E5%8D%B1%E5%AE%B3.%E4%BB%A5%E4%BD%8E%E6%88%90%E6%9C%AC%E9%9D%9E%E6%8E%A5%E8%A7%A6%E5%BC%8F%E7%9A%84%E6%89%8B%E6%9C%BA%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E4%B8%BA%E5%9F%BA%E7%A1%80%2C%E9%80%9A%E8%BF%87%E5%AF%B9%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E7%89%B9%E6%80%A7%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2C%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E8%9E%8D%E5%90%88Encoder-Decoder%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%8EAt-tention%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E7%9A%84%E5%9C%A8%E7%BA%BF%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95.%E8%AF%A5%E6%96%B9%E6%B3%95%E7%94%B1%E5%9F%BA%E4%BA%8ELSTM%20%28long%20short-term%20memory%29%E7%9A%84Encoder-Decoder%2CAt-tention%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%9F%BA%E4%BA%8ESVM%20%28support%20vector,machine%29%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A83%20%E4%B8%AA%E6%A8%A1%E5%9D%97%E6%9E%84%E6%88%90.%E8%AF%A5%E7%B3%BB%E7%BB%9F%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%E5%8C%85%E6%8B%AC%3A%E8%BE%93%E5%85%A5%E7%BC%96%E7%A0%81%2C%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%AD%A6%E4%B9%A0%2C%E7%89%B9%E5%BE%81%E8%A7%A3%E7%A0%81%2C%E5%BA%8F%E5%88%97%E9%87%8D%E6%9E%84%2C%E6%AE%8B%E5%B7%AE%E8%AE%A1%E7%AE%97%E4%B8%8E%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E5%88%86%E7%B1%BB%E7%AD%896%20%E4%B8%AA%E6%AD%A5%E9%AA%A4.%E8%AF%A5%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E5%88%A9%E7%94%A8%E8%87%AA%E7%84%B6%E9%A9%BE%E9%A9%B6%E6%9D%A1%E4%BB%B6%E4%B8%8B%E6%89%80%E9%87%87%E9%9B%86%E7%9A%84%E6%89%8B%E6%9C%BA%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%AE%9E%E9%AA%8C.%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E8%A1%A8%E6%98%8E%3A%E2%91%A0%20%E6%89%8B%E6%9C%BA%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95%E5%AF%B9%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%85%B7%E5%A4%87%E6%9C%89%E6%95%88%E6%80%A7%3B%E2%91%A1%20%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E5%BF%85%E7%84%B6%E4%BC%9A%E9%80%A0%E6%88%90%E6%95%B0%E6%8D%AE%E5%BC%82%E5%B8%B8%E6%B3%A2%E5%8A%A8%3B%E2%91%A2%20Attention%E6%9C%BA%E5%88%B6%E6%9C%89%E5%8A%A9%E4%BA%8E%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%95%88%E6%9E%9C%2C%E5%AF%B9%E6%89%80%E6%8F%90%E5%87%BA%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%86%E5%88%AB%E5%87%86%E7%A1%AE%E7%8E%87F1-score%E4%B8%BA0.717%2C%E4%B8%8E%E7%BB%8F%E5%85%B8%E5%90%8C%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%BE%83%2C%E5%87%86%E7%A1%AE%E7%8E%87%E5%BE%97%E5%88%B0%E6%98%BE%E8%91%97%E6%8F%90%E5%8D%87%3B%E2%91%A3%E5%AF%B9%E4%BA%8E%E6%B1%BD%E8%BD%A6%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E6%9D%A5%E8%AF%B4%2CSVM%E6%AF%94Logistic%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E5%85%B7%E6%9C%89%E6%9B%B4%E4%BC%98%E8%B6%8A%E7%9A%84%E8%AF%86%E5%88%AB%E6%95%88%E6%9E%9C.). 申请编号（专利号）：CN202111675120.8 应用日期：2021-12-31), Pengfei Wei, Lingdong Kong, Xinghua Qu, **Yi Ren**, et al.

