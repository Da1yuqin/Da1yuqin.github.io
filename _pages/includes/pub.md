
# 📝 Publications 
## 国际会议
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TechRxiv 2024</div><img src='images/hvsurvey_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[TechRxiv24][Human Motion Video Generation: A Survey](https://arxiv.org/pdf/2403.06189)

<a href="https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file"><img src="https://img.shields.io/github/stars/Winn1y/Awesome-Human-Motion-Video-Generation?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file"><img src="https://img.shields.io/github/forks/Winn1y/Awesome-Human-Motion-Video-Generation?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file) [[Project Page]](https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file)

Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,__Yuqin Dai__,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu. 
- 数字人视频生成领域综述. 
- 总结了超过300篇最新数字人视频生成了领域相关文献的内容.
- 总结了现有数字人视频生成领域范式.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv 2024</div><img src='images/TCDiff_mainfig.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[arXiv24][Harmonious Group Choreography with Trajectory-Controllable Diffusion](https://arxiv.org/pdf/2403.06189)

<a href="https://github.com/Da1yuqin/TCDiff"><img src="https://img.shields.io/github/stars/Da1yuqin/TCDiff?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Da1yuqin/TCDiff"><img src="https://img.shields.io/github/forks/Da1yuqin/TCDiff?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Da1yuqin/TCDiff) [[Project Page]](https://wanluzhu.github.io/TCDiffusion/)

__Yuqin Dai__, Wanlu Zhu, Ronghui Li, Zeping Ren, Xiangzheng Zhou, Xiu Li, Jun Li, Jian Yang. 
- 发现并提出领域内存在的问题: 舞者混淆(Dancer Ambiguity)现象. 为后续研究提供指引与思路. 
- TCDiff 是当前 SOTA 的多人舞蹈生成模型, 能够较好的解决舞者混淆(Dancer Ambiguity)现象.
- 提出了 Footwork Adaptor 模块, 能有效缓解多人舞蹈生成中的脚步滑动问题(Foot Slide).
- 提出了 Fusion Projection 插件, 该插件占用较小的计算资源, 能够有效解决舞者混淆(Dancer Ambiguity)现象

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP2024 Oral</div><img src='images/mainfig_text2avatar.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ICASSP24 Oral][Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute](https://ieeexplore.ieee.org/document/10446237)

 Chaoqun Gong\*, __Yuqin Dai__\*, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu L. 
- Text2Avatar 是第一个基于复杂耦合的输入文本提示生成逼真风格的 3D Avatar 的模型，实现了多属性可控和逼真的 3D 人头像生成。
- Text2Avatar 模型基于 3D-Aware GAN(NeRF-Based), 使用 GAN-Inversion based 的方式实现文本对齐, 巧妙化解了当前文本标注的写实风格三维 Avatar 数据集缺失的问题.
- 提出 Multi-Modal Encoder, 能够作为插件服务于其他模型, 具有很强的可扩展性.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP2024</div><img src='images/DanceControl_mainfig.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ICASSP24][EXPLORING MULTI-MODAL CONTROL IN MUSIC-DRIVEN DANCE GENERATION](https://ieeexplore.ieee.org/document/10447825)

Ronghui Li\*, __Yuqin Dai__\*, Yachao Zhang, Jun Li, Jian Yang, Jie Guo, Xiu Li. 
- 提出了第一个统一的框架，能够生成高质量的舞蹈动作，并支持多模态控制，包括同时进行流派控制、语义控制和空间控制。
- 模型能够进行音乐跨模态舞蹈生成(Music2Dance), 基于 VQ-GPT 架构, 能够一次生成长达 16s 的舞蹈动作, 并能通过自回归的方式对生成内容进行拓展. 

</div>
</div>

## 中文论文
[基于Encoder-Decoder注意力网络的异常驾驶行为在线识别方法](http://bzxb.cqut.edu.cn/paperinfo.aspx?paperid=11163)，兵器装备工程学报2023(核心期刊) 
- 唐坤(指导老师)，__戴语琴__，徐永能，郭唐仪，邵飞
- 我们提出了一个实时监控驾驶员异常驾驶行为的框架。与现有依赖大量昂贵高精度传感器的方法不同，我们只使用手机数据进行异常驾驶行为检测，使我们的方法更加实用。

[转动惯量平行轴定理验证实验的改进方案](https://dxwl.bnu.edu.cn/CN/10.16854%20/j.cnki.1000-0712.190392)，大学物理2020(核心期刊)
- 闫敏，__戴语琴__，袁俊，王晓雄
- 我们找到了一种比目前的教学方法更准确的方法，可以显着减少实验中测量刚体转动惯量时引入的误差量。这将有助于大学生更有效地进行实验，更好地理解实验原理。


## 专利
- 唐坤， 杨力， __戴语琴__，郭唐仪，徐永能. [基于编码器-解码器注意力网络和LSTM的异常驾驶在线识别方法](https://xueshu.baidu.com/usercenter/paper/show?paperid=180r0c70ap6800w0hu2t0jn0je340821#:~:text=%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E6%98%AF%E8%BD%A6%E8%BE%86%E5%AE%89%E5%85%A8%E8%BF%90%E8%A1%8C%E7%9A%84%E9%87%8D%E5%A4%A7%E5%A8%81%E8%83%81%2C%E5%85%B6%E5%AF%B9%E4%BA%BA%E5%91%98%E4%B8%8E%E7%89%A9%E8%B5%84%E7%9A%84%E5%AE%89%E5%85%A8%E9%AB%98%E6%95%88%E6%8A%95%E9%80%81%E9%80%A0%E6%88%90%E4%B8%A5%E9%87%8D%E5%8D%B1%E5%AE%B3.%E4%BB%A5%E4%BD%8E%E6%88%90%E6%9C%AC%E9%9D%9E%E6%8E%A5%E8%A7%A6%E5%BC%8F%E7%9A%84%E6%89%8B%E6%9C%BA%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E4%B8%BA%E5%9F%BA%E7%A1%80%2C%E9%80%9A%E8%BF%87%E5%AF%B9%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E7%89%B9%E6%80%A7%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2C%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E8%9E%8D%E5%90%88Encoder-Decoder%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%8EAt-tention%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E7%9A%84%E5%9C%A8%E7%BA%BF%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95.%E8%AF%A5%E6%96%B9%E6%B3%95%E7%94%B1%E5%9F%BA%E4%BA%8ELSTM%20%28long%20short-term%20memory%29%E7%9A%84Encoder-Decoder%2CAt-tention%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%9F%BA%E4%BA%8ESVM%20%28support%20vector,machine%29%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A83%20%E4%B8%AA%E6%A8%A1%E5%9D%97%E6%9E%84%E6%88%90.%E8%AF%A5%E7%B3%BB%E7%BB%9F%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%E5%8C%85%E6%8B%AC%3A%E8%BE%93%E5%85%A5%E7%BC%96%E7%A0%81%2C%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%AD%A6%E4%B9%A0%2C%E7%89%B9%E5%BE%81%E8%A7%A3%E7%A0%81%2C%E5%BA%8F%E5%88%97%E9%87%8D%E6%9E%84%2C%E6%AE%8B%E5%B7%AE%E8%AE%A1%E7%AE%97%E4%B8%8E%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E5%88%86%E7%B1%BB%E7%AD%896%20%E4%B8%AA%E6%AD%A5%E9%AA%A4.%E8%AF%A5%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E5%88%A9%E7%94%A8%E8%87%AA%E7%84%B6%E9%A9%BE%E9%A9%B6%E6%9D%A1%E4%BB%B6%E4%B8%8B%E6%89%80%E9%87%87%E9%9B%86%E7%9A%84%E6%89%8B%E6%9C%BA%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%AE%9E%E9%AA%8C.%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E8%A1%A8%E6%98%8E%3A%E2%91%A0%20%E6%89%8B%E6%9C%BA%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95%E5%AF%B9%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%85%B7%E5%A4%87%E6%9C%89%E6%95%88%E6%80%A7%3B%E2%91%A1%20%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E5%BF%85%E7%84%B6%E4%BC%9A%E9%80%A0%E6%88%90%E6%95%B0%E6%8D%AE%E5%BC%82%E5%B8%B8%E6%B3%A2%E5%8A%A8%3B%E2%91%A2%20Attention%E6%9C%BA%E5%88%B6%E6%9C%89%E5%8A%A9%E4%BA%8E%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%95%88%E6%9E%9C%2C%E5%AF%B9%E6%89%80%E6%8F%90%E5%87%BA%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%86%E5%88%AB%E5%87%86%E7%A1%AE%E7%8E%87F1-score%E4%B8%BA0.717%2C%E4%B8%8E%E7%BB%8F%E5%85%B8%E5%90%8C%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%BE%83%2C%E5%87%86%E7%A1%AE%E7%8E%87%E5%BE%97%E5%88%B0%E6%98%BE%E8%91%97%E6%8F%90%E5%8D%87%3B%E2%91%A3%E5%AF%B9%E4%BA%8E%E6%B1%BD%E8%BD%A6%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E6%9D%A5%E8%AF%B4%2CSVM%E6%AF%94Logistic%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E5%85%B7%E6%9C%89%E6%9B%B4%E4%BC%98%E8%B6%8A%E7%9A%84%E8%AF%86%E5%88%AB%E6%95%88%E6%9E%9C.). 申请编号（专利号）：CN202111675120.8 应用日期：2021-12-31), Pengfei Wei, Lingdong Kong, Xinghua Qu, **Yi Ren**, et al.

