
# 📝 Publications 
## 一作/共一论文(7)
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/EviNoteRAG_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[arXiv25][EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes](https://arxiv.org/abs/2509.00877)
 
<a href="https://github.com/Da1yuqin/EviNoteRAG"><img src="https://img.shields.io/github/stars/Da1yuqin/EviNoteRAG?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Da1yuqin/EviNoteRAG"><img src="https://img.shields.io/github/forks/Da1yuqin/EviNoteRAG?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Da1yuqin/EviNoteRAG)

**Yuqin Dai**\*, Guoqing Wang\*, Yuan Wang\*, Kairan Dou, Kaichen Zhou, Zhanwei Zhang, Shuo Yang, Fei Tang, Jun Yin, Pengyu Zeng, Zhenzhe Ying, Can Yi, Changhua Meng, Yuchen Zhou, Yongliang Shen, Shuai Lu
-我们提出 EviNote-RAG 框架，将“检索-回答”模式改为“检索-笔记-回答”，提升信息提炼和推理可靠性。
- 引入类人类的检索式摘要机制，生成支持性证据笔记（SENs），突出关键信息和不确定点，减少噪音、提升聚焦。
- 方法在多个问答基准上达到 SOTA，不仅效果显著提升，还大幅增强训练稳定性和效率，例如在 HotpotQA、Bamboogle、2Wiki 上分别提升 F1 20% (+0.093)、40% (+0.151)、91% (+0.256)。
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/WebFilter_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[arXiv25][Careful Queries, Credible Results: Teaching RAG Models Advanced Web Search Tools with Reinforcement Learning](https://arxiv.org/pdf/2508.07956)
 
<a href="https://github.com/GuoqingWang1/WebFilter"><img src="https://img.shields.io/github/stars/GuoqingWang1/WebFilter?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/GuoqingWang1/WebFilter"><img src="https://img.shields.io/github/forks/GuoqingWang1/WebFilter?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/GuoqingWang1/WebFilter)

 **Yuqin Dai**\*, Shuo Yang\*, Guoqing Wang\*, Yong Deng, Zhanwei Zhang, Jun Yin, Pengyu Zeng, Zhenzhe Ying, Changhua Meng, Can Yi, Yuchen Zhou, Weiqiang Wang, Shuai Lu
- 我们提出了 WebFilter 框架，将检索过程建模为马尔可夫决策过程，并通过强化学习训练大语言模型使用高级网页搜索操作符，从而在真实网络环境中有效过滤虚假信息。
- 设计了信息过滤奖励策略，结合“来源限制奖励”和“检索精度奖励”，同时优化查询行为与检索结果质量，显著提高了检索精准度与可信度。
- 实验表明，WebFilter 在多项问答基准上取得了最优性能，高级搜索操作符的使用率由 10% 提升至 75%，并在域内与跨域任务中均展现出强泛化能力。
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/TCDiffpp_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[arXiv25][TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography](https://arxiv.org/pdf/2506.18671)
 
<a href="https://github.com/Da1yuqin/TCDiffpp"><img src="https://img.shields.io/github/stars/Da1yuqin/TCDiffpp?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Da1yuqin/TCDiffpp"><img src="https://img.shields.io/github/forks/Da1yuqin/TCDiffpp?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Da1yuqin/TCDiff) [[Project Page]](https://da1yuqin.github.io/TCDiffpp.website/)

 **Yuqin Dai**\*, Wanlu Zhu\*, Ronghui Li, Xiu Li, Zhenyu Zhang, Jun Li,Jian Yang
- 我们提出TCDiff++, 一种端到端版本的群舞生成模型。
- 我们引入位置嵌入和一致性损失，防止碰撞并保持合理间距。
- 模型加入换位信息和脚步自适应器，减少脚滑并提升一致性。
- 优化长时生成效果，提出长序列采样与解码器，优化长舞蹈生成的连贯性。
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2025</div><img src='images/mindaligner_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ICML25][MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data](https://arxiv.org/pdf/2502.05034)
 
<a href="https://github.com/Da1yuqin/MindAligner"><img src="https://img.shields.io/github/stars/Da1yuqin/MindAligner?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Da1yuqin/MindAligner"><img src="https://img.shields.io/github/forks/Da1yuqin/MindAligner?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Da1yuqin/TCDiff) 
 
**Yuqin Dai**\*, Zhouheng Yao\*, Chunfeng Song, Qihao Zheng, Weijian Mai, Kunyu Peng, Shuai Lu, Wanli Ouyang, Jian Yang, Jiamin Wu.
- 我们提出了MindAligner，这是第一个显式的大脑对齐框架，能够在数据有限的情况下实现跨个体的视觉解码和大脑功能分析。
- 我们提出了一种大脑转移矩阵，用于建立不同个体之间的细粒度功能对应关系。该矩阵通过大脑功能对齐模块进行优化，采用多层次对齐损失实现软性跨个体映射。
- 实验表明，MindAligner在视觉解码任务中，只有6%的模型参数被学习时，便超越了现有的最先进方法。
- 我们进行了跨个体的大脑功能可视化研究，发现早期视觉皮层在不同个体间活动相似，而与记忆和空间导航相关的高级视觉皮层则表现出显著的个体间差异。
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI2025 Oral🥇</div><img src='images/TCDiff_mainfig.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[AAAI25 Oral][Harmonious Group Choreography with Trajectory-Controllable Diffusion](https://arxiv.org/pdf/2403.06189)

<a href="https://github.com/Da1yuqin/TCDiff"><img src="https://img.shields.io/github/stars/Da1yuqin/TCDiff?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Da1yuqin/TCDiff"><img src="https://img.shields.io/github/forks/Da1yuqin/TCDiff?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Da1yuqin/TCDiff) [[Project Page]](https://wanluzhu.github.io/TCDiffusion/)

__Yuqin Dai__, Wanlu Zhu, Ronghui Li, Zeping Ren, Xiangzheng Zhou, Xiu Li, Jun Li, Jian Yang. 
- 发现并提出领域内存在的问题: 舞者混淆(Dancer Ambiguity)现象. 为后续研究提供指引与思路. 
- TCDiff 是当前 SOTA 的多人舞蹈生成模型, 能够较好的解决舞者混淆(Dancer Ambiguity)现象.
- 提出了 Footwork Adaptor 模块, 能有效缓解多人舞蹈生成中的脚步滑动问题(Foot Slide).
- 提出了 Fusion Projection 插件, 该插件占用较小的计算资源, 能够有效解决舞者混淆(Dancer Ambiguity)现象

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP2024 Oral🥇</div><img src='images/mainfig_text2avatar.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ICASSP24 Oral][Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute](https://ieeexplore.ieee.org/document/10446237)

 Chaoqun Gong\*, __Yuqin Dai__\*, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu L. 
- Text2Avatar 是第一个基于复杂耦合的输入文本提示生成逼真风格的 3D Avatar 的模型，实现了多属性可控和逼真的 3D 人头像生成。
- Text2Avatar 模型基于 3D-Aware GAN(NeRF-Based), 使用 GAN-Inversion based 的方式实现文本对齐, 巧妙化解了当前文本标注的写实风格三维 Avatar 数据集缺失的问题.
- 提出 Multi-Modal Encoder, 能够作为插件服务于其他模型, 具有很强的可扩展性.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP2024</div><img src='images/DanceControl_mainfig.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ICASSP24][EXPLORING MULTI-MODAL CONTROL IN MUSIC-DRIVEN DANCE GENERATION](https://ieeexplore.ieee.org/document/10447825)

Ronghui Li\*, __Yuqin Dai__\*, Yachao Zhang, Jun Li, Jian Yang, Jie Guo, Xiu Li. 
- 提出了第一个统一的框架，能够生成高质量的舞蹈动作，并支持多模态控制，包括同时进行流派控制、语义控制和空间控制。
- 模型能够进行音乐跨模态舞蹈生成(Music2Dance), 基于 VQ-GPT 架构, 能够一次生成长达 16s 的舞蹈动作, 并能通过自回归的方式对生成内容进行拓展. 

</div>
</div>

## 参与工作（6）
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/logic_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[arXiv25][Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models](https://arxiv.org/pdf/2508.11317)

Yuchen Zhou, Jiayu Tang, Shuo Yang, Xiaoyan Xiao, **Yuqin Dai**, Wenhao Yang, Chao Gou, Xiaobo Xia, Tat-Seng Chua
- 提出 LogicBench 基准：构建了一个涵盖 9 类逻辑关系、4 种应用场景和 2 种评测任务的综合基准，包含超过 5 万对逻辑视觉-语言样本，用于系统性评估多模态大模型的逻辑理解能力。
- 开展系统性诊断评测：首次系统性分析多模态大模型在逻辑推理方面的表现，揭示了其在理解复杂逻辑结构时存在的显著“逻辑盲点”和固有限制。
- 提出 LogicCLIP 框架：设计了一种新型训练方法，有效增强模型的逻辑敏感性。实验表明，LogicCLIP 在多个领域显著提升逻辑理解能力的同时，还保持甚至超越了在标准视觉-语言基准上的表现。
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/atom_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[arXiv25][Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/pdf/2508.12800)

<a href="https://github.com/antgroup/Research-Venus"><img src="https://img.shields.io/github/stars/antgroup/Research-Venus?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/antgroup/Research-Venus"><img src="https://img.shields.io/github/forks/antgroup/Research-Venus?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/antgroup/Research-Venus) 

Yong Deng∗, Guoqing Wang∗, Zhenzhe Ying∗, Xiaofeng Wu∗, Jinzhen Lin, Wenwen Xiong, **Yuqin Dai**, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Changhua Meng
- 提出全新的“原子思维”范式：将大模型的推理过程拆解为细粒度的功能单元，从而引导模型进行更清晰、更深入的推理。
- 设计原子思维奖励机制（ATR）及课程式聚合策略：通过将 ATR 与最终结果奖励结合，缓解了策略优化中的梯度冲突和奖励稀疏问题。
- 构建 Atom-Searcher 框架并验证效果：基于原子思维与奖励聚合策略，提出了一个新的强化学习框架 Atom-Searcher，在七个基准测试上超越现有最优方法，并展现出多方面优势。
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/RAMA_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[arXiv25][RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/pdf/2507.09174?)

Shuo Yang, Zijian Yu, Zhenzhe Ying, **Yuqin Dai**, Guoqing Wang, Jun Lan, Jinfeng Xu, Jinze Li, Edith C.H. Ngai
- 提出了一个多模态融合框架，将视觉特征与语言模型有效结合，实现跨模态推理与生成能力的显著提升。
- 引入动态注意力机制，根据上下文自适应地调整视觉与文本信息的权重，从而提升了模型在多模态任务中的鲁棒性与泛化性。
- 在多项多模态基准测试（如图文匹配、视觉问答等）中，该方法均取得了优于现有方法的性能表现，验证了其有效性与先进性。
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACMMM25 Datasets</div><img src='images/realfactbentch_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ACMMM25 Datasets][RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking](https://arxiv.org/pdf/2506.12538)
 
<a href="https://github.com/kalendsyang/RealFactBench"><img src="https://img.shields.io/github/stars/kalendsyang/RealFactBench?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Dkalendsyang/RealFactBench"><img src="https://img.shields.io/github/forks/kalendsyang/RealFactBench?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/kalendsyang/RealFactBench) 
 
Shuo Yang, **Yuqin Dai**, Guoqing Wang, Xinran Zheng, Jinfeng Xu, Jinze Li, Zhenzhe Ying, Weiqiang Wang, Edith CH Ngai.
- 我们提出 RealFactBench 基准测试集：构建了一个涵盖知识验证、谣言检测和事件核查等多种真实世界任务的综合性基准，用于评估大语言模型（LLMs）和多模态大模型（MLLMs）的事实核查能力。
- 引入新的评估指标 Unknown Rate (UnR)：该指标用于更细致地衡量模型在不确定性处理方面的表现，帮助评估模型在保守性与自信程度之间的平衡。
- 开展大规模实证研究：在7个典型LLMs和4个MLLMs上进行了系统评估，揭示了当前模型在事实核查任务中的局限性，并为后续研究提供了有价值的洞察。
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2025</div><img src='images/hvsurvey_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[TPAMI25][Human Motion Video Generation: A Survey](https://www.techrxiv.org/users/836049/articles/1228135-human-motion-video-generation-a-survey)

<a href="https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file"><img src="https://img.shields.io/github/stars/Winn1y/Awesome-Human-Motion-Video-Generation?style=social" alt="GitHub Stars" /></a>
<a href="https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file"><img src="https://img.shields.io/github/forks/Winn1y/Awesome-Human-Motion-Video-Generation?style=social" alt="GitHub Forks" /></a>
[[Github]](https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file) [[Project Page]](https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation?tab=readme-ov-file)

Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,__Yuqin Dai__,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu. 
- 数字人视频生成领域综述. 
- 总结了超过300篇最新数字人视频生成了领域相关文献的内容.
- 总结了现有数字人视频生成领域范式.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL25 Oral🥇, SAC Highlights⭐</div><img src='images/FloorPlan-LLaMa_mainfig.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[ACL25 Oral][FloorPlan-LLaMa: Aligning Architects’ Feedback and Domain Knowledge in Architectural Floor Plan Generation](https://aclanthology.org/2025.acl-long.331/)

Jun Yin, Pengyu Zeng, Haoyuan Sun, **Yuqin Dai**, Han Zheng, Miao Zhang, Yachao Zhang, Shuai Lu
- 提出了ArchiMetricsNet数据集与FloorPlan-MPS评价模型：首次构建了一个包含功能性、流线性和整体性评估得分的楼面图数据集，并配有详细的文本分析，用以更贴近建筑专业知识地评估生成结果。
- 开发了FloorPlan-LLaMa生成模型并引入RLHF机制：设计了基于自回归框架的楼面图生成模型FloorPlan-LLaMa，并通过引入FloorPlan-MPS作为奖励模型，借助人类反馈强化学习（RLHF）机制使模型更符合建筑师的专业偏好。
- 实验证明方法优于现有基线并获专业认可：在文本条件和类别条件的生成任务中均优于现有基线模型，且经专业建筑师验证，其生成结果更为合理，契合人类设计偏好。
</div>
</div>


<!-- 
## 中文论文
[基于Encoder-Decoder注意力网络的异常驾驶行为在线识别方法](http://bzxb.cqut.edu.cn/paperinfo.aspx?paperid=11163)，兵器装备工程学报2023(核心期刊) 
- 唐坤(指导老师)，__戴语琴__，徐永能，郭唐仪，邵飞
- 我们提出了一个实时监控驾驶员异常驾驶行为的框架。与现有依赖大量昂贵高精度传感器的方法不同，我们只使用手机数据进行异常驾驶行为检测，使我们的方法更加实用。

[转动惯量平行轴定理验证实验的改进方案](https://dxwl.bnu.edu.cn/CN/10.16854%20/j.cnki.1000-0712.190392)，大学物理2020(核心期刊)
- 闫敏，__戴语琴__，袁俊，王晓雄
- 我们找到了一种比目前的教学方法更准确的方法，可以显着减少实验中测量刚体转动惯量时引入的误差量。这将有助于大学生更有效地进行实验，更好地理解实验原理。



## 专利
- 唐坤， 杨力， __戴语琴__，郭唐仪，徐永能. [基于编码器-解码器注意力网络和LSTM的异常驾驶在线识别方法](https://xueshu.baidu.com/usercenter/paper/show?paperid=180r0c70ap6800w0hu2t0jn0je340821#:~:text=%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E6%98%AF%E8%BD%A6%E8%BE%86%E5%AE%89%E5%85%A8%E8%BF%90%E8%A1%8C%E7%9A%84%E9%87%8D%E5%A4%A7%E5%A8%81%E8%83%81%2C%E5%85%B6%E5%AF%B9%E4%BA%BA%E5%91%98%E4%B8%8E%E7%89%A9%E8%B5%84%E7%9A%84%E5%AE%89%E5%85%A8%E9%AB%98%E6%95%88%E6%8A%95%E9%80%81%E9%80%A0%E6%88%90%E4%B8%A5%E9%87%8D%E5%8D%B1%E5%AE%B3.%E4%BB%A5%E4%BD%8E%E6%88%90%E6%9C%AC%E9%9D%9E%E6%8E%A5%E8%A7%A6%E5%BC%8F%E7%9A%84%E6%89%8B%E6%9C%BA%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E4%B8%BA%E5%9F%BA%E7%A1%80%2C%E9%80%9A%E8%BF%87%E5%AF%B9%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E7%89%B9%E6%80%A7%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2C%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E8%9E%8D%E5%90%88Encoder-Decoder%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%8EAt-tention%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E7%9A%84%E5%9C%A8%E7%BA%BF%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95.%E8%AF%A5%E6%96%B9%E6%B3%95%E7%94%B1%E5%9F%BA%E4%BA%8ELSTM%20%28long%20short-term%20memory%29%E7%9A%84Encoder-Decoder%2CAt-tention%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%9F%BA%E4%BA%8ESVM%20%28support%20vector,machine%29%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A83%20%E4%B8%AA%E6%A8%A1%E5%9D%97%E6%9E%84%E6%88%90.%E8%AF%A5%E7%B3%BB%E7%BB%9F%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%E5%8C%85%E6%8B%AC%3A%E8%BE%93%E5%85%A5%E7%BC%96%E7%A0%81%2C%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%AD%A6%E4%B9%A0%2C%E7%89%B9%E5%BE%81%E8%A7%A3%E7%A0%81%2C%E5%BA%8F%E5%88%97%E9%87%8D%E6%9E%84%2C%E6%AE%8B%E5%B7%AE%E8%AE%A1%E7%AE%97%E4%B8%8E%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E5%88%86%E7%B1%BB%E7%AD%896%20%E4%B8%AA%E6%AD%A5%E9%AA%A4.%E8%AF%A5%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E5%88%A9%E7%94%A8%E8%87%AA%E7%84%B6%E9%A9%BE%E9%A9%B6%E6%9D%A1%E4%BB%B6%E4%B8%8B%E6%89%80%E9%87%87%E9%9B%86%E7%9A%84%E6%89%8B%E6%9C%BA%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%AE%9E%E9%AA%8C.%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E8%A1%A8%E6%98%8E%3A%E2%91%A0%20%E6%89%8B%E6%9C%BA%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95%E5%AF%B9%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%85%B7%E5%A4%87%E6%9C%89%E6%95%88%E6%80%A7%3B%E2%91%A1%20%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E5%BF%85%E7%84%B6%E4%BC%9A%E9%80%A0%E6%88%90%E6%95%B0%E6%8D%AE%E5%BC%82%E5%B8%B8%E6%B3%A2%E5%8A%A8%3B%E2%91%A2%20Attention%E6%9C%BA%E5%88%B6%E6%9C%89%E5%8A%A9%E4%BA%8E%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%95%88%E6%9E%9C%2C%E5%AF%B9%E6%89%80%E6%8F%90%E5%87%BA%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%86%E5%88%AB%E5%87%86%E7%A1%AE%E7%8E%87F1-score%E4%B8%BA0.717%2C%E4%B8%8E%E7%BB%8F%E5%85%B8%E5%90%8C%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%BE%83%2C%E5%87%86%E7%A1%AE%E7%8E%87%E5%BE%97%E5%88%B0%E6%98%BE%E8%91%97%E6%8F%90%E5%8D%87%3B%E2%91%A3%E5%AF%B9%E4%BA%8E%E6%B1%BD%E8%BD%A6%E5%BC%82%E5%B8%B8%E9%A9%BE%E9%A9%B6%E8%A1%8C%E4%B8%BA%E6%9D%A5%E8%AF%B4%2CSVM%E6%AF%94Logistic%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E5%85%B7%E6%9C%89%E6%9B%B4%E4%BC%98%E8%B6%8A%E7%9A%84%E8%AF%86%E5%88%AB%E6%95%88%E6%9E%9C.). 申请编号（专利号）：CN202111675120.8 应用日期：2021-12-31), Pengfei Wei, Lingdong Kong, Xinghua Qu, **Yi Ren**, et al.
-->

